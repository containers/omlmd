{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Getting Started","text":""},{"location":"#oci-artifact-for-ml-model-metadata","title":"OCI Artifact for ML model &amp; metadata","text":"<p>This project is a collection of blueprints, patterns and toolchain (in the form of python SDK and CLI) to leverage OCI Artifact and containers for ML model and metadata.</p>"},{"location":"#installation","title":"Installation","text":"<p>In your Python environment, use:</p> <pre><code>pip install omlmd\n</code></pre> <p>Why do I need a Python environment?</p> <p>This SDK follows the same prerequisites as InstructLab and is intented to offer Pythonic way to create OCI Artifact for ML model and metadata. For general CLI tools for containers, we invite you to checkout Podman and all the Containers toolings.</p>"},{"location":"#push","title":"Push","text":"<p>Store ML model file <code>model.joblib</code> and its metadata in the OCI repository at <code>localhost:8080</code>:</p> PythonCLI <pre><code>from omlmd.helpers import Helper\n\nomlmd = Helper()\nomlmd.push(\"localhost:8080/matteo/ml-artifact:latest\", \"model.joblib\", name=\"Model Example\", author=\"John Doe\", license=\"Apache-2.0\", accuracy=9.876543210)\n</code></pre> <pre><code>omlmd push localhost:8080/mmortari/mlartifact:v1 model.joblib --metadata md.json --plain-http\n</code></pre>"},{"location":"#pull","title":"Pull","text":"<p>Fetch everything in a single pull:</p> PythonCLI <pre><code>omlmd.pull(target=\"localhost:8080/matteo/ml-artifact:latest\", outdir=\"tmp/b\")\n</code></pre> <pre><code>omlmd pull localhost:8080/mmortari/mlartifact:v1 -o tmp/a --plain-http\n</code></pre> <p>Or fetch only the ML model assets:</p> PythonCLI <pre><code>omlmd.pull(target=\"localhost:8080/matteo/ml-artifact:latest\", outdir=\"tmp/b\", media_types=[\"application/x-mlmodel\"])\n</code></pre> <pre><code>omlmd pull localhost:8080/mmortari/mlartifact:v1 -o tmp/b --media-types \"application/x-mlmodel\" --plain-http\n</code></pre>"},{"location":"#custom-pull-just-metadata","title":"Custom Pull: just metadata","text":"<p>The features can be composed in order to expose higher lever capabilities, such as retrieving only the metadata informatio. Implementation intends to follow OCI-Artifact convention</p> PythonCLI <pre><code>md = omlmd.get_config(target=\"localhost:8080/matteo/ml-artifact:latest\")\nprint(md)\n</code></pre> <pre><code>omlmd get config localhost:8080/mmortari/mlartifact:v1 --plain-http\n</code></pre>"},{"location":"#crawl","title":"Crawl","text":"<p>Client-side crawling of metadata.</p> <p>Note: Server-side analogous coming soon/reference in blueprints.</p> PythonCLI <pre><code>crawl_result = omlmd.crawl([\n    \"localhost:8080/matteo/ml-artifact:v1\",\n    \"localhost:8080/matteo/ml-artifact:v2\",\n    \"localhost:8080/matteo/ml-artifact:v3\"\n])\n</code></pre> <pre><code>omlmd crawl localhost:8080/mmortari/mlartifact:v1 localhost:8080/mmortari/mlartifact:v2 localhost:8080/mmortari/mlartifact:v3 --plain-http\n</code></pre>"},{"location":"#example-query","title":"Example query","text":"<p>Demonstrate integration of crawling results with querying (in this case using jQ)</p> <p>Of the crawled ML OCI artifacts, which one exhibit the max accuracy?</p> PythonCLI <pre><code>import jq\njq.compile( \"max_by(.config.customProperties.accuracy).reference\" ).input_text(crawl_result).first()\n</code></pre> <pre><code>omlmd crawl --plain-http \\\n    localhost:8080/mmortari/mlartifact:v1 \\\n    localhost:8080/mmortari/mlartifact:v2 \\\n    localhost:8080/mmortari/mlartifact:v3 \\\n    | jq \"max_by(.config.customProperties.accuracy).reference\"\n</code></pre>"},{"location":"#to-be-continued","title":"To be continued...","text":""},{"location":"glossary/","title":"Glossary","text":"<p>Termonology used in the scope of OCI Artifact for ML model &amp; metadata (this project).</p>"},{"location":"glossary/#lorem-ipsum","title":"Lorem ipsum","text":""},{"location":"glossary/#coming-soon","title":"Coming soon...","text":""},{"location":"overview/","title":"Overview","text":"<p>This project aims to demonstrate how ML model and related metadata can be stored as an OCI Artifact and then leverage the vast OCI/Container ecosystem and infrastructure for comprehensive MLOps.</p> <p>For the impatients</p> <p>You can jump straight to Demo 1.</p> <p>The proposed OCI Artifact for ML model and metadata can be organized and then stored in OCI compliant registries with a format similar to the following:</p> <p></p> <p>Please note in the first case, both the actual ML model file (<code>mnist.onnx</code> file) and its metadata, can be stored in an OCI artifact.</p> <p>In the second case, the OCI artifact does not contain the actual model file, as its metadata is referring a model already existing on external storage, such as HuggingFace hub, but similarly it could be pointing at a git repository with lfs support, an S3 bucket coordinates, etc.</p> <p>Note</p> <p>The OCI artifact without model and only metadata is considered highly experimental.</p> <p>That represents an unified format to represent the ML model and its metadata as OCI Artifact, while keeping some flexibility on the actual storage solution (external reference). The first provides additional advantages such as an unified distribution mechanism for local, cached, disconnected environments, enabled by the OCI-Dist spec and related infrastructure.</p>"},{"location":"overview/#example","title":"Example","text":"<p>As a concrete example, let's consider a manifest for an OCI Artifact for ML model and metadata where:</p> <ul> <li>the Manifest's <code>.config</code> points to the layer containing metadata, following OCI Image spec recommendation</li> <li>there is another layer for the same metadata but in yaml format, for convenience</li> <li>the model file is contained in a dedicated layer(s)</li> <li>for convenience, the first level of the metadata are also projected to Manifest's Annotations. Since OCI Image annotation are only <code>string-string</code> value pairs, we are using media-type conventions for JSON structures. Please notice this is NOT intended to natively query in the scope of this project (more below).</li> </ul> <p>The manifest is relized with:</p> <pre><code>{\n  \"schemaVersion\": 2,\n  \"mediaType\": \"application/vnd.oci.image.manifest.v1+json\",\n  \"config\": {\n    \"mediaType\": \"application/x-config\",\n    \"size\": 269,\n    \"digest\": \"sha256:3a0f78f88e0fe9d9cad48f7d279a1911434183e484dab54e405dcdf75502b2bc\"\n  },\n  \"layers\": [\n    {\n      \"mediaType\": \"application/x-artifact\",\n      \"size\": 3315,\n      \"digest\": \"sha256:3e6605871679b65ceaa0aac78c65a4e019db50d59b39ad88952c659605398140\",\n      \"annotations\": {\n        \"org.opencontainers.image.title\": \"model.joblib\"\n      }\n    },\n    {\n      \"mediaType\": \"application/x-config\",\n      \"size\": 269,\n      \"digest\": \"sha256:3a0f78f88e0fe9d9cad48f7d279a1911434183e484dab54e405dcdf75502b2bc\",\n      \"annotations\": {\n        \"org.opencontainers.image.title\": \"model_metadata.omlmd.json\"\n      }\n    },\n    {\n      \"mediaType\": \"application/x-config\",\n      \"size\": 187,\n      \"digest\": \"sha256:09fcf26fa55e340ed3d374cf6677418c671934106003dd6267a96cb76e4e728e\",\n      \"annotations\": {\n        \"org.opencontainers.image.title\": \"model_metadata.omlmd.yaml\"\n      }\n    }\n  ],\n  \"annotations\": {\n    \"name\": \"Model Example\",\n    \"author\": \"John Doe\",\n    \"customProperties+json\": \"{\\\"license\\\": \\\"Apache-2.0\\\", \\\"accuracy\\\": 0.9777777777777777}\"\n  }\n}\n</code></pre> <p>and can be inspected using skopeo or any other standard OCI tooling.</p>"},{"location":"overview/#integration-with-traditional-metadata-regisries","title":"Integration with traditional metadata regisries","text":"<p>From the perspective of this project, Metadata Registry (such as Kubeflow Model Registry) and OCI format are meant to be complementary, integrated solutions: a Data Scientist can choose to keep using existing MR for the iteration cycles during the inner-loop of the MLOps workflow storing the ad-interim models on any storage such as S3, PVs, etc.; when the resulting model start to get satisfactory, this OCI format could be more relevant, to integrate with existing infrastructure. To make this transition smooth, a Python-sdk and cli tooling must be made available (scope of this project).</p> <p>During the transition from more inner-loop centric to outer-loop phases of the MLOps life cycle, integrating with OCI infrastructure would allow creating workflow to produce, provenance and lineage additional Container artifacts, such as Bootable container images, ModelCar images for KServe, etc. (see section below about integration with pipelines)</p>"},{"location":"overview/#integration-with-pipelines","title":"Integration with pipelines","text":"<p>Since OCI container registries are already well integrated with CI/CD systems, pipelines, and workflow engines, this represent an unique opportunity to leverage existing integrations for the pupose of automated ML training workflows, including signature, attestation, provenance, etc.</p> <p></p> <p>In this example, a Data Scientist/MLOps using Jupyter notebook leverage a designated Python sdk producing OCI Artifact to store the intended ML model and related metadata; in an alternate scenario, this is achieved from other AI related tool such as Podman Desktop AI.</p> <p>A notification mechanism is triggered on artifact push from the OCI repository, or by alternative discovery, in order to start a CI workflow producing additional OCI Container images, wrapping the original ML model. These additional container images could be a KServe ModelCar, or a Bootable container (a bootable linux, ML server and the ML model itself), etc. The goal of these container images is to effectively serve for inference the original ML model, providing in practice a serving inference runtime. These images would [OCI 1.1] make Subject of the initial ML model OCI Artifact reference, and in turn become Referrers (please note: OCI Subject/Referrer mechanism only works if using same oci-registry and same repository, ie: \u201cmyorg/myartifact\u201d inside \u201cquay.io\u201d etc).</p> <p>Note: the above diagram does not represent signing artifacts to keep the diagram simple, but signatures can be done in order to progressively sign the supply-chain of these artifacts as they are getting produced and made available.</p>"},{"location":"overview/#integration-with-kep-4639","title":"Integration with KEP-4639","text":"<p>Kubernetes effort are in place to support direct mounting of OCI Artifact as a Volume in a running Containers; this would make it possible (at least in theory at this stage) to directly serve in Kubernetes an OCI Artifact containing just the ML model asset, without a strong requirement of having to build around it a ModelCar or a runnable container image to self-serve it.</p> <p>For more information: see here and KEP-4639 link.</p> <p>Specifically for KServe, until KEP-4639 will be available, we have a concrete and running example to use OCI artifact spec and fetch the model in KServe using ClusterStorageContainer API to extend the storage initializer (as a complementary approach to KServe\u2019s ModelCar).</p>"},{"location":"overview/#integration-with-signature-provenance","title":"Integration with Signature, Provenance","text":"<p>This infrastructure can be also combined with Sigstore/CoSign, in order to build on a \u201ctrusted model secure supply chain\u201d workflow.</p> <p>First, the ML model and (replicable) metadata get signed as OCI Artifact on a OCI registry; then a downstream workflow kicks-in, to produce a container image \u201caround\u201d the ML model; this workflow can verify the signatures correspond to the expected one (\u201cI am using a ML model asset which has not been tampered with\u201d) and sign the resulting container image. As part of this latter signature, can include relevant pointers (in the form of OCI referrer or other metadata) to the original ML model.</p> <p>This in turn provides a signature chain, supporting more than Lineage, also Provenance.</p> <p>See also Demo 3 in this project.</p>"},{"location":"overview/#integration-with-opa","title":"Integration with OPA","text":"<p>This infrastructure can be also combined with OPA rules to enforce a given governance policy in the organization, for example:</p> <pre><code># still to be fully tested\npackage kubernetes.admission\n\nimport data.kubernetes.admission\n\ndeny[{\"msg\": msg}] {\n  input.request.kind.kind == \"InferenceService\"\n  storageUri := input.request.object.spec.predictor.model.storageUri\n  not is_valid_storageUri(storageUri)\n  msg := sprintf(\"storageUri %v is not valid. It must use the 'oci' protocol and come from the 'myregistry' registry.\", [storageUri])\n}\n\nis_valid_storageUri(uri) {\n  parts := split(uri, \"://\")\n  count(parts) == 2\n\n  protocol := parts[0]\n  protocol == \"oci\"\n\n  rest := parts[1]\n  registry := split(rest, \"/\")[0]\n  registry == \"myregistry\"\n}\n</code></pre> <p>to ensure the KServe Isvc is admitted only if coming for a specified OCI registry, i.e.: only deploy ML models for inference which are coming from the enterprise OCI registry.</p> <p>Note: this is already possible in general, just providing an explicit example here for completeness.</p> <p>TODO: expand on Enterprise Contract, in progress</p>"},{"location":"overview/#integration-with-confidential-containers","title":"Integration with Confidential Containers","text":"<p>This infrastructure can be also combined with Confidential Containers, so that OCI Artifacts containing the ML model asset (or any container image \u201cbuilt around it\u201d) can be consumed or deployed only by trusted clusters that meet predefined policies or security requirements. On an untrusted cluster/system, the Confidential Containers will prevent the provisioning and execution of the OCI container, thereby safeguarding the ML model asset\u2019s confidentiality. This capability is very interesting especially for hybrid cloud environments, where ensuring security and privacy across infrastructures (and potentially multiple tenants) is of vital importance.</p>"},{"location":"overview/#integration-with-querying","title":"Integration with Querying","text":"<p>As OCI 1.1 specification is being leveraged, the infrastructure and tooling proposed in this document should also focus on offering tailored query capabilities, in two ways:</p> <ul> <li>as a \u201ccrawler\u201d mechanism: i.e.: on the OCI/ORAS client-side, to address a query directly or for end-user to store results in a index of choice</li> <li>as a \u201cbackend\u201d mechanism i.e.: on OCI repositories allowing concepts of artifacts-plugin,  as a way to address the user query from the server side</li> </ul> <p>In this context, it will be meaningful to assess opportunities to use the same query language (or \u201cengine\u201d); for example, assess \u201cjson path\u201d or equivalent for both crawler and backend using PostgreSQL.</p>"},{"location":"use-cases/","title":"Use Cases","text":"<p>Coming soon...</p>"},{"location":"appendixes/appendix-links/","title":"All links","text":""},{"location":"appendixes/appendix-links/#appendix-all-links","title":"Appendix \"All links\"","text":"<p>This is a collection of all the links from this website:</p> <ul> <li>https://containers.github.io/omlmd</li> <li>https://docs.sigstore.dev/</li> <li>https://github.com/containers/omlmd</li> <li>https://github.com/containers/omlmd/actions/workflows/build.yaml</li> <li>https://github.com/containers/omlmd/actions/workflows/e2e.yaml</li> <li>https://github.com/kubeflow/community/pull/682#issuecomment-2130072374</li> <li>https://github.com/kubernetes/enhancements/pull/4639</li> <li>https://github.com/lampajr/oci-storage-initializer/blob/main/GET_STARTED.md</li> <li>https://github.com/opencontainers/image-spec/blob/main/manifest.md#guidelines-for-artifact-usage</li> <li>https://jqlang.github.io/jq</li> <li>https://pypi.org/project/omlmd</li> <li>https://www.cncf.io/projects/confidential-containers</li> <li>https://www.kubeflow.org/docs/components/model-registry</li> <li>https://www.openpolicyagent.org/</li> <li>https://www.youtube.com/watch?v=W4GwIRPXE8E&amp;list=PLdbdefeRIj9SRbg6Hkr15GeyPH0qpk_ww</li> </ul>"},{"location":"demos/demo/","title":"Demo 1: Introduction","text":""},{"location":"demos/demo/#demo-1-introduction","title":"Demo 1: Introduction","text":"<p>This is a work in progress demonstrator; after showing a very simple ML training, we package the ML model assets and relevant metadata as OCI Artifact. We push the resulting artifact to a specified OCI repository. We demonstrate how to pull the artifact using standard clients or a customized client. We demonstrate how to build on top of the provided features, to provide new capabilities, such as custom Pull or local Crawling of metadata for Querying.</p>"},{"location":"demos/demo/#model-training","title":"Model Training","text":"<p>We simulate (poorly!) a ML model training and we persist the resulting model in a joblib file.</p> <pre><code>import joblib\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\n\nX, y = datasets.load_iris(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 1, stratify = y)\nsvc_linear = SVC(kernel=\"linear\", probability=True)\nsvc_linear.fit(X_train, y_train)\n\ny_pred = svc_linear.predict(X_test)\naccuracy_value = accuracy_score(y_test, y_pred)\nprint(\"accuracy:\", accuracy_value)\n</code></pre> <pre><code>accuracy: 0.9777777777777777\n</code></pre> <pre><code>with open(\"model.joblib\", 'wb') as fo:  \n   joblib.dump(svc_linear, fo)\n\n%ls -lA model*\n</code></pre> <pre><code>-rw-r--r--@ 1 mmortari  staff  3299 Jun 17 10:22 model.joblib\n</code></pre>"},{"location":"demos/demo/#oci-artifact","title":"OCI Artifact","text":"<p>Let's leverage OCI-Artifact and OCI-Dist to warehouse our ML model and its metadata.</p> <pre><code>from omlmd.helpers import Helper\n\nomlmd = Helper()\nomlmd.push(\"localhost:8080/matteo/ml-artifact:latest\", \"model.joblib\", name=\"Model Example\", author=\"John Doe\", license=\"Apache-2.0\", accuracy=accuracy_value)\n</code></pre> <pre><code>Successfully pushed localhost:8080/matteo/ml-artifact:latest\n</code></pre> Zot Quay <p>Demonstrate pull with vanilla OCI-compliant clients</p> <pre><code>from oras.provider import Registry\n\noras_registry = Registry(insecure=True)\noras_registry.pull(target=\"localhost:8080/matteo/ml-artifact:latest\", outdir=\"tmp/a\")\n\n%ls -lA tmp/a\n</code></pre> <pre><code>total 24\n-rw-r--r--@ 1 mmortari  staff  3299 Jun 17 10:22 model.joblib\n-rw-r--r--@ 1 mmortari  staff   269 Jun 17 10:22 model_metadata.oml.json\n-rw-r--r--@ 1 mmortari  staff   187 Jun 17 10:22 model_metadata.oml.yaml\n</code></pre> <p>Demonstrate custom pull, filtering to download only ML artifact and nothing else</p> <pre><code>omlmd.pull(target=\"localhost:8080/matteo/ml-artifact:latest\", outdir=\"tmp/b\", media_types=[\"application/x-artifact\"])\n\n%ls -lA tmp/b\n</code></pre> <pre><code>total 8\n-rw-r--r--@ 1 mmortari  staff  3299 Jun 17 10:22 model.joblib\n</code></pre> <p>Demonstrate custom fetch of metadata layer (following OCI-Artifact conventions)</p> <pre><code>print(omlmd.get_config(target=\"localhost:8080/matteo/ml-artifact:latest\"))\n</code></pre> <pre><code>{\"reference\":\"localhost:8080/matteo/ml-artifact:latest\", \"config\": {\n    \"name\": \"Model Example\",\n    \"description\": null,\n    \"author\": \"John Doe\",\n    \"customProperties\": {\n        \"license\": \"Apache-2.0\",\n        \"accuracy\": 0.9777777777777777\n    },\n    \"uri\": null,\n    \"model_format_name\": null,\n    \"model_format_version\": null\n} }\n</code></pre>"},{"location":"demos/demo/#crawl-oci-artifacts","title":"Crawl OCI-Artifacts","text":"<p>Demonstrator of client-side crawling. This is only a demonstrator, working on analogous concept server-side (beyond OCI specification, but integrating with it).</p> <pre><code># data prep (simulated): store in OCI 3 tags, with different `accuracy` metadata\nomlmd.push(\"localhost:8080/matteo/ml-artifact:v1\", \"model.joblib\", accuracy=.85, name=\"Model Example\", author=\"John Doe\", license=\"Apache-2.0\")\nomlmd.push(\"localhost:8080/matteo/ml-artifact:v2\", \"model.joblib\", accuracy=.90, name=\"Model Example\", author=\"John Doe\", license=\"Apache-2.0\")\nomlmd.push(\"localhost:8080/matteo/ml-artifact:v3\", \"model.joblib\", accuracy=.95, name=\"Model Example\", author=\"John Doe\", license=\"Apache-2.0\")\n</code></pre> <pre><code>Successfully pushed localhost:8080/matteo/ml-artifact:v1\nSuccessfully pushed localhost:8080/matteo/ml-artifact:v2\nSuccessfully pushed localhost:8080/matteo/ml-artifact:v3\n</code></pre> Zot Quay <pre><code>crawl_result = omlmd.crawl([\n    \"localhost:8080/matteo/ml-artifact:v1\",\n    \"localhost:8080/matteo/ml-artifact:v2\",\n    \"localhost:8080/matteo/ml-artifact:v3\"\n])\n</code></pre> <p>Demonstrate integration of crawling results with querying (in this case using jQ)</p> <p>Of the crawled ML OCI artifacts, which one exhibit the max accuracy?</p> <pre><code>import jq\njq.compile( \"max_by(.config.customProperties.accuracy).reference\" ).input_text(crawl_result).first()\n</code></pre> <pre><code>'localhost:8080/matteo/ml-artifact:v3'\n</code></pre>"},{"location":"demos/demo/#from-ml-model-in-oci-artifact-to-modelcar","title":"from ML model in OCI Artifact \u2192 to ModelCar","text":"<p>ModelCar's Dockerfile:</p> <pre><code>FROM ghcr.io/oras-project/oras:v1.2.0 as builder\n\nRUN oras pull quay.io/mmortari/ml-iris:v1 \n\n\nFROM busybox\n\nRUN mkdir /models &amp;&amp; chmod 775 /models\nCOPY --from=builder /workspace /models/\n</code></pre> <pre><code>!podman build --load -t mmortari/ml-iris:v1-modelcar -f Containerfile.modelcar .\n# !podman push --tls-verify=false mmortari/ml-iris:v1-modelcar localhost:8080/matteo/ml-iris:v1-modelcar\n!podman push mmortari/ml-iris:v1-modelcar quay.io/mmortari/ml-iris:v1-modelcar\n</code></pre> <pre><code>[1/2] STEP 1/2: FROM ghcr.io/oras-project/oras:v1.2.0 AS builder\n[1/2] STEP 2/2: RUN oras pull quay.io/mmortari/ml-iris:v1 \n--&gt; Using cache 7feb1e5fb58481657bd017001bd1f8ce7f930041f522c29ffcee44bc346bf99c\n--&gt; 7feb1e5fb584\n[2/2] STEP 1/3: FROM busybox\n[2/2] STEP 2/3: RUN mkdir /models &amp;&amp; chmod 775 /models\n--&gt; Using cache 4c41b98df27a711498d7e585a7e6a13cc660dc86dc2a30f45fd4d869e5b65091\n--&gt; 4c41b98df27a\n[2/2] STEP 3/3: COPY --from=builder /workspace /models/\n--&gt; Using cache b6a5b03fd625e3a49fd6bd104d250c6efe1be53f48f23db54a1714513e9eb954\n[2/2] COMMIT mmortari/ml-iris:v1-modelcar\n--&gt; b6a5b03fd625\nSuccessfully tagged localhost/mmortari/ml-iris:v1-modelcar\nSuccessfully tagged localhost/matteo/ml-iris:v1-modelcar\nb6a5b03fd625e3a49fd6bd104d250c6efe1be53f48f23db54a1714513e9eb954\nGetting image source signatures\nCopying blob sha256:e5744b46b6c629c1861eb438aca266a1a170a519f080db5885cc4e672cd78d1b\nCopying blob sha256:8e13bc96641a6983e79c9569873afe5800b0efb3993c3302763b9f5bc5fb8739\nCopying blob sha256:a1d8fcd2d8014f56ebfd7710bc9487fe01364b8007acca13d75a0127e7f8247a\nCopying config sha256:b6a5b03fd625e3a49fd6bd104d250c6efe1be53f48f23db54a1714513e9eb954\nWriting manifest to image destination\n</code></pre> local Quay Quay.io"},{"location":"demos/demo/#from-modelcar-to-bootc-image-linuxservermodelcar","title":"from ModelCar \u2192 to BootC image (linux+server+model[/car])","text":"<p>bootc containerfile (snippet):</p> <pre><code>FROM quay.io/centos-bootc/centos-bootc:stream9\n# ...\n\n# Add quadlet files to setup system to automatically run AI application on boot\nCOPY quadlet/sklearn.kube quadlet/sklearn.yaml /usr/share/containers/systemd/\n\n# Prepull the model, model_server &amp; application images to populate the system.\n# Comment the pull commands to keep bootc image smaller.\n# The quadlet .image file added above pulls following images with service startup\nRUN podman pull --root /usr/lib/containers/storage docker.io/kserve/sklearnserver:latest\nRUN podman pull --root /usr/lib/containers/storage quay.io/mmortari/ml-iris:v1-modelcar\n\n# ...\n</code></pre> <pre><code>!podman build --build-arg \"SSHPUBKEY=$(cat ~/.ssh/id_rsa.pub)\" \\\n       --security-opt label=disable \\\n       --cap-add SYS_ADMIN \\\n       -f Containerfile.bootc \\\n       -t mmortari/ml-iris:v1-bootc .\n!podman push mmortari/ml-iris:v1-bootc quay.io/mmortari/ml-iris:v1-bootc\n</code></pre> <pre><code>STEP 1/9: FROM quay.io/centos-bootc/centos-bootc:stream9\nSTEP 2/9: ARG SSHPUBKEY\n--&gt; Using cache 523580821612112581608763e3943eb40817089f87b690dac045459c0b14fb99\n--&gt; 523580821612\nSTEP 3/9: RUN set -eu; mkdir -p /usr/ssh &amp;&amp;     echo 'AuthorizedKeysFile /usr/ssh/%u.keys .ssh/authorized_keys .ssh/authorized_keys2' &gt;&gt; /etc/ssh/sshd_config.d/30-auth-system.conf &amp;&amp;     echo ${SSHPUBKEY} &gt; /usr/ssh/root.keys &amp;&amp; chmod 0600 /usr/ssh/root.keys\n--&gt; Using cache 3359a78489d3e4eca5921449532819c1b234660be8ac46f3752dad6ee8989eff\n--&gt; 3359a78489d3\nSTEP 4/9: COPY quadlet/sklearn.kube quadlet/sklearn.yaml /usr/share/containers/systemd/\n--&gt; Using cache 5dbe59af0d46b95e74577ce99172e11917622f847d00e4b231cb3d10a937d74a\n--&gt; 5dbe59af0d46\nSTEP 5/9: RUN sed -i -e '/additionalimage.*/a \"/usr/lib/containers/storage\",'         /etc/containers/storage.conf\n--&gt; Using cache e16046b72ce01887444619469f31aa4d758cb7dc8b07c51dd7848cc452349df9\n--&gt; e16046b72ce0\nSTEP 6/9: VOLUME /var/lib/containers\n--&gt; Using cache 8c0ce999a83d0f12b4484c750749d8bf7483ebd43862a14dd833f7c91416297e\n--&gt; 8c0ce999a83d\nSTEP 7/9: RUN podman pull --root /usr/lib/containers/storage docker.io/kserve/sklearnserver:latest\n--&gt; Using cache 1102e2d0a0bc9d1295d6d78fa44b44774fd365c3c44dcb719c4bbdf549bd81fb\n--&gt; 1102e2d0a0bc\nSTEP 8/9: RUN podman pull --root /usr/lib/containers/storage quay.io/mmortari/ml-iris:v1-modelcar\n--&gt; Using cache 8915d99264260de1e7f8b5e4c438e3cb9d66f6ce79fab5c5a7f47608ea71a654\n--&gt; 8915d9926426\nSTEP 9/9: RUN podman system reset --force 2&gt;/dev/null\n--&gt; Using cache f2b145347580340b1257bafbd2d0dc4b78452af539c1aa13e4dc7a01b0181c51\nCOMMIT mmortari/ml-iris:v1-bootc\n--&gt; f2b145347580\nSuccessfully tagged localhost/mmortari/ml-iris:v1-bootc\nSuccessfully tagged localhost/matteo/ml-bootc:latest\nf2b145347580340b1257bafbd2d0dc4b78452af539c1aa13e4dc7a01b0181c51\nGetting image source signatures\nCopying blob sha256:159348fa9cfbb75c5cb57e9fac24b9f28477412149e901bdadb909bfaeb84dad\nCopying blob sha256:9a1a0862c7696bd2e36bf7aad37f9e59a17de5e9ee17e4e7b9e9decc965476e7\nCopying blob sha256:8f4a35e515241f6ad7d2201a35e5ff05332e9fbcae37df036c075817e9b1804b\n\n...\n\nCopying blob sha256:4c718200cc93786f4b77f1e43fb517f87e45ff88544789a3390a55c63ec510ec\nCopying blob sha256:c6d68a01008a8b18cc588c38dda4043cf9b1a6ba672a791bc69c796da386e2ec\nCopying blob sha256:c7af602eb478cda4aa9841fb7049eaa3c55a3ed8b347d5a95956c783fe59d472\nCopying config sha256:f2b145347580340b1257bafbd2d0dc4b78452af539c1aa13e4dc7a01b0181c51\nWriting manifest to image destination\n</code></pre> <p>Now the bootc container image is available:</p> <p></p> <p>We could also make a Virtual Machine out of it:</p> <p></p> <p>I could run the Virtual Machine and it would serve my model:</p> <p></p> <p>and I could interact with it to make Inference:</p> <p></p>"},{"location":"demos/demo2/","title":"Demo 2: more on ModelCar","text":"<p>TODO: revise this summary</p> <p>In this follow-up demo, we focus on using OCI Artifacts for Machine Learning model assets and their metadata, specifically within the context of \"ModelCar\". We start by recapping the previous setup where we wrapped a machine learning model as an OCI artifact and then as a ModelCar. Today, we'll explore three demos: using the ModelCar in a traditional KServe setup, using it within a KServe raw environment, and directly utilizing the OCI artifact in KServe with a custom storage initializer.</p> <p>First, we demonstrate using the ModelCar in a traditional KServe setup. We begin by interacting with the machine learning model locally to understand the input and output behavior. After preparing a KServe cluster, we deploy the ModelCar and verify it by making predictions using test data. The predictions align with our expectations, demonstrating that the ModelCar works correctly within the KServe environment.</p> <p>Next, we move to a KServe Raw environment to show the versatility of the ModelCar. We prepare a KServe Raw-enabled cluster and deploy the ModelCar following similar steps as in the traditional setup. We interact with the deployed model in KServe Raw, confirming that it predicts the same class values for given inputs, just as it did in the KServe setup. This consistency underscores the ModelCar's compatibility across different serving environments.</p> <p>In the final demo, we highlight the power of composition by using the OCI Artifact directly in KServe. We define a custom storage initializer for OCI Artifacts using the provided library. This allows us to deploy the machine learning model directly from the OCI artifact without wrapping it in a ModelCar. We apply the custom storage initializer in our Kubernetes cluster and interact with the model, achieving the same prediction results as before. This demonstrates the flexibility and compositional power of the underlying OCI infrastructure.</p> <p>Throughout these demos, we illustrate how the ModelCar and OCI artifacts can be seamlessly integrated and utilized within different Kubernetes-based serving environments. The consistent prediction results across local, KServe, and KServe Raw setups validate the robustness of this approach. Additionally, the use of a custom storage initializer showcases the adaptability of the system to handle OCI artifacts directly.</p> <p>In conclusion, we have demonstrated the workflow from wrapping a machine learning model as an OCI Artifact, deploying it as a ModelCar, and using it in different serving environments. We also showcased the direct use of OCI Artifacts with a custom storage initializer, highlighting the flexibility and scalability offered by KServe and Kubernetes. These demos provide a comprehensive view of how to leverage OCI Artifacts for efficient and scalable machine learning model deployment.</p>"},{"location":"demos/demo3/","title":"Demo 3: Signature and Attestation","text":"<p>TODO revise this summary</p> <p>In this demo, we expand on using OCI Artifacts and underlying infrastructure for storing and distributing machine learning model assets, and their metadata. We focus on Signatures and Attestations, which are crucial for building a trusted model supply chain. Ensuring a trusted software supply chain is vital, especially in MLOps, where model Provenance and Lineage are essential to confirm that models put into production are secure and traceable.</p> <p>We demonstrate how to train a machine learning model in a pipeline that provides both a signature and attestation for the resulting OCI artifact. This means that in the OCI repository, we will have the machine learning model, its metadata, and cryptographic signatures to ensure the integrity of the pipeline. The attestation helps us understand the steps and processes that led to the creation of the OCI artifact, supporting transparency and trust in the model's origin.</p> <p>Our demo uses Tekton Chains to generate cryptographic signatures and attestations, but the underlying principles apply to any standard technology such as <code>in-toto</code> or <code>SLSA</code>. We show the training process using a simple Python script that loads data, trains a model, computes accuracy, and then pushes the result to a container repository. This example illustrates the ease of integrating cryptographic signature support into a machine learning pipeline.</p> <p>We then go through the steps of creating signing keys, setting up authentication for the OCI repository, and running the pipeline. The process includes loading data, training the model, and pushing the artifact to the repository, where it is signed and attested. The signatures and attestations are verified using the public key, ensuring the artifact's integrity and providing detailed information on how it was created.</p> <p>Finally, we summarize the importance of using well-known infrastructure for producing and verifying machine learning models and their metadata. The demonstrated infrastructure is composable, allowing for parameterization of input and output, and adaptable to various CI/CD providers. By using these foundational building blocks, we can better manage model provenance and lineage, ensuring a secure and trusted machine learning model supply chain.</p>"}]}